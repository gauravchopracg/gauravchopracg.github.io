---
layout: post
title: "Gated Recurrent Unit"
---

In this tutorial, you will discover Gated Recurrent Unit which is modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems.

The formula for computing the activations at time t of RNN

$a^<t>$ = g($W_a$[$a^<t-1>$, $x^<t>$] + $b_a$)

g is tanh activation function

The GRU unit is going to have a new variable called c, which stands for cell, for memory cell. And what memory cell will do is it will provide a bit of memory to remember. So, at time t the memory cell will have some value c of t. And GRU unit will actually output an activation value a of t that's equal to c of t.

$c^<t>$ = $a^<t>$

$c^<t>$ = tanh($W_c$[$c^<t-1>, x^<t>] + $b_c$)
