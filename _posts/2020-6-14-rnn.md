---
layout: post
title: "Recurrent Neural Network"
---

In this tutorial, you will discover, what is Recurrent Neural Network, types of RNN and how to build a Recurrent Neural Network,

*You can run the tutorials yourself at here.*

*The GitHub links for this tutorial are: Browse, Zip.*

Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have "memory". They can read inputs $x^{\langle t \rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. 


### Different types of RNNs

* Many-to-Many architecture because input sequence has many inputs as a sequence and the outputs sequence is also has many outputs. This type of architecture is used in Machine translation or named entity recognition

* Many-to-One architecture because as many inputs, it inputs many words and then it just outputs one number. This type of architecture is used in Sentiment classification

* One-to-One architecture one number input x and one number output y. This type of architecture is used in. 

* One-to-Many architecture : music generation : output a set of notes corresponding to a piece of music and the input x could be maybe just an integer, telling it what genre of music you want or what is the first note of the music you want, and if you don't want to input anything, x could be a null input, could always be the vector zeroes as well.

When input length and output length are different many-to-many for an application like machine translation, the number of words in the input sentence, say a french sentence, and the number of words in the output sentence, say the translation into English, those sentences could be different lengths. This network has two distinct parts. There's the encoder, which takes as input, say a French sentence, and then, there's is a decoder, which having read in the sentence, outputs the translation into a different language


### Step-by-Step process to build a basic Recurrent Neural Network

**Notation:**
* Superscript $[l]$ denotes an object associated with the $l^{th}$ layer.
* Superscript $(i)$ denotes an object associated with the $i^{th}$ example.
* Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step.
* Subscript $i$ denotes the $i^{th} entry of a vector

Consider an example: $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step<4>, and 5th entry in the vector.

A recurrent neural network can be seen as the repeated use of a single cell. We are first going to implement the computations for a single time-step.

**Steps to implement single step RNN forward**:

1. Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$.
2. Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided the function `softmax`.
3. Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in a `cache`.
4. Return $a^{\langle t \rangle}$ , $\hat{y}^{\langle t \rangle}$ and `cache`

```python
    # compute next activation state using the formula given above
    a_next = np.tanh(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)
    # compute output of the current cell using the formula given above
    yt_pred = softmax(np.dot(Wya,a_next)+by)
```

We will start by computing the backward pass for the basic RNN-cell

**Steps to implement single step RNN backward**:

To compute the rnn_cell_backward you can utilize the following equations. It is a good exercise to derive them by hand. Here, $*$ denotes element-wise multiplication while the absence of a symbol indicates matrix multiplication.

$a^{\langle t \rangle} = \tanh(W_{ax} x^{\langle t \rangle} + W_{aa} a^{\langle t-1 \rangle} + b_{a})\tag{-}$
 
$\displaystyle \frac{\partial \tanh(x)} {\partial x} = 1 - \tanh^2(x) \tag{-}$
 
$\displaystyle  {dW_{ax}} = da_{next} * ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) x^{\langle t \rangle T}\tag{1}$

$\displaystyle dW_{aa} = da_{next} * (( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )  a^{\langle t-1 \rangle T}\tag{2}$

$\displaystyle db_a = da_{next} * \sum_{batch}( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{3}$
 
$\displaystyle dx^{\langle t \rangle} = da_{next} * { W_{ax}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{4}$
  
$\displaystyle da_{prev} = da_{next} * { W_{aa}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{5}$


The results can be computed directly by implementing the equations above. However, the above can optionally be simplified by computing 'dz' and utlilizing the chain rule.  
This can be further simplified by noting that $\tanh(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})$ was computed and saved in the forward pass. 

```python
    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h
    daraw = (1 - a * a) * da # backprop through tanh nonlinearity
```

### Application:
* Speech Recognition
* Music Generation
* Sentiment Classification
* DNA Sequence Analysis
* Machine Translation
* Video Activity Recognition
* Named Entity Recognition


### Vanishing gradients with RNNs
It is observed that RNN is not very good at capturing very long-term dependencies as it leads to vanishing gradients problem, for the outputs of the errors associated with the later time steps affect the computations that are earlier. It turns out that vanishing gradients tends to be the bigger problem with training RNNs, although when exploding gradients happens, it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. And it also turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs, or not a numbers, meaning results of a numerical overflow in your neural network computation.

One solution to deal with exploding gradients is to apply gradient clipping, that is to look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vector so that is not too big. Hence, there are clips according to some maximum value.
