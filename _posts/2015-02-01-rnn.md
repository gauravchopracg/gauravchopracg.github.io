---
layout: post
title: "Recurrent Neural Network"
---

In this tutorial, you will discover, what is Recurrent Neural Network, types of RNN and how to build a Recurrent Neural Network,

Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have "memory". They can read inputs $x^<t>$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. 

### Step-by-Step process to build a basic Recurrent Neural Network

**Notation:**
* Superscript $[l]$ denotes an object associated with the $l^{th}$ layer.
* Superscript $(i)$ denotes an object associated with the $i^{th}$ example.
* Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step.
* Subscript $i$ denotes the $i^{th} entry of a vector

Consider an example: $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step<4>, and 5th entry in the vector.

A recurrent neural network can be seen as the repeated use of a single cell. We are first going to implement the computations for a single time-step.

![alt text](
_____________________________________
<img src="images/rnn_step_forward_figure2_v3a.png" style="width:700px;height:300px;">
<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\langle t \rangle}$ (current input) and $a^{\langle t - 1\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\langle t \rangle}$ which is given to the next RNN cell and also used to predict $\hat{y}^{\langle t \rangle}$ </center></caption>

Steps to implement single step RNN forward
1. Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$.
2. Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y}$
3. Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in a `cache`.
4. Return $a^{\langle t \rangle}$, $\hat{y}^{\langle t \rangle}$ and `cache`

Let's take a look at random calculation with assumption:
_________________

Basic RNN  backward pass

We will start by computing the backward pass for the basic RNN-cell and then in the following sections, iterate through the cells.

<img src="images/rnn_backward_overview_3a_1.png" style="width:500;height:300px;"> <br>
<caption><center> **Figure 6**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the time steps of the RNN by following the chain-rule from calculus. Internal to the cell, the chain-rule is also used to calculate $(\frac{\partial J}{\partial W_{ax}},\frac{\partial J}{\partial W_{aa}},\frac{\partial J}{\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. The operation can utilize the cached results from the forward path. </center></caption>

Recall from lecture, the shorthand for the partial derivative of cost relative to a variable is dVariable. For example, $\frac{\partial J}{\partial W_{ax}}$ is $dW_{ax}$. This will be used throughout the remaining sections.


<img src="images/rnn_cell_backward_3a_4.png" style="width:500;height:300px;"> <br>
<caption><center> **Figure 7**: This implementation of rnn_cell_backward does not include the output dense layer and softmax which are included in rnn_cell_forward. 

$da_{next}$ is $\frac{\partial{J}}{\partial a^{\langle t \rangle}}$ and includes loss from previous stages and current stage output logic. This addition will be part of your implementation of rnn_backward.  </center></caption>

##### Equations
To compute the rnn_cell_backward you can utilize the following equations. It is a good exercise to derive them by hand. Here, $*$ denotes element-wise multiplication while the absence of a symbol indicates matrix multiplication.

$a^{\langle t \rangle} = \tanh(W_{ax} x^{\langle t \rangle} + W_{aa} a^{\langle t-1 \rangle} + b_{a})\tag{-}$
 
$\displaystyle \frac{\partial \tanh(x)} {\partial x} = 1 - \tanh^2(x) \tag{-}$
 
$\displaystyle  {dW_{ax}} = da_{next} * ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) x^{\langle t \rangle T}\tag{1}$

$\displaystyle dW_{aa} = da_{next} * (( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )  a^{\langle t-1 \rangle T}\tag{2}$

$\displaystyle db_a = da_{next} * \sum_{batch}( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{3}$
 
$\displaystyle dx^{\langle t \rangle} = da_{next} * { W_{ax}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{4}$
  
$\displaystyle da_{prev} = da_{next} * { W_{aa}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{5}$


#### Implementing rnn_cell_backward
The results can be computed directly by implementing the equations above. However, the above can optionally be simplified by computing 'dz' and utlilizing the chain rule.  
This can be further simplified by noting that $\tanh(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})$ was computed and saved in the forward pass. 

To calculate dba, the 'batch' above is a sum across the horizontal (axis= 1) axis. Note that you should use the keepdims = True option.

It may be worthwhile to review Course 1 [Derivatives with a computational graph](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph)  through  [Backpropagation Intuition](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional), which decompose the calculation into steps using the chain rule.  
Matrix vector derivatives are described [here](http://cs231n.stanford.edu/vecDerivs.pdf), though the equations above incorporate the required transformations.

Note rnn_cell_backward does __not__ include the calculation of loss from $y \langle t \rangle$, this is incorporated into the incoming da_next. This is a slight mismatch with rnn_cell_forward which includes a dense layer and softmax. 

Note: in the code:  
$\displaystyle dx^{\langle t \rangle}$ is represented by dxt,   
$\displaystyle d W_{ax}$ is represented by dWax,   
$\displaystyle da_{prev}$ is represented by da_prev,    
$\displaystyle dW_{aa}$ is represented by dWaa,   
$\displaystyle db_{a}$ is represented by dba,   
dz is not derived above but can optionally be derived by students to simplify the repeated calculations.

To compute RNN backward, we will start with

### Different types of RNNs

* Many-to-Many architecture because input sequence has many inputs as a sequence and the outputs sequence is also has many outputs

* Many-to-One architecture because as many inputs, it inputs many words and then it just outputs one number.

* One-to-One architecture one number input x and one number output y

* One-to-Many architecture : music generation : output a set of notes corresponding to a piece of music and the input x could be maybe just an integer, telling it what genre of music you want or what is the first note of the music you want, and if you don't want to input anything, x could be a null input, could always be the vector zeroes as well.

When input length and output length are different many-to-many for an application like machine translation, the number of words in the input sentence, say a french sentence, and the number of words in the output sentence, say the translation into English, those sentences could be different lengths. This network has two distinct parts. There's the encoder, which takes as input, say a French sentence, and then, there's is a decoder, which having read in the sentence, outputs the translation into a different language


### Application:
* Speech Recognition
* Music Generation
* Sentiment Classification
* DNA Sequence Analysis
* Machine Translation
* Video Activity Recognition
* Named Entity Recognition

### Vanishing gradients with RNNs
It is observed that RNN is not very good at capturing very long-term dependencies as it leads to vanishing gradients problem, for the outputs of the errors associated with the later time steps affect the computations that are earlier. It turns out that vanishing gradients tends to be the bigger problem with training RNNs, although when exploding gradients happens, it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. And it also turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs, or not a numbers, meaning results of a numerical overflow in your neural network computation.

One solution to deal with exploding gradients is to apply gradient clipping, that is to look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vector so that is not too big. Hence, there are clips according to some maximum value.

### Language Model

To build such a model using an RNN you would first need a training set comprising a large corpus of english text after that first thing you would do is tokenize this sentence than means you would form a vocabulary and then map each of these words to one-hot vectors, alter indices in your vocabulary. Other thing you might also want to do is model when sentences end. So another common thing to do is add an extra token called a EOS that stands for End Of Sentence that can help you figure out when a sentence ends. Now, one other detail would be what if some words in your training set, are not in your vocabulary, then term that will be in the sentence will be replaced by a unique token called UNK that stands for unknown words

Now let's build an RNN to model the chance of these different sequences.

To train the neural network we are going to define the cost function at a certain time, t, if the true word was yt and the new networks soft max predicted some y hat t, then this the soft max loss function

Sample novel sequences

Sample a sequence from a trained RNN

Character-level language model
